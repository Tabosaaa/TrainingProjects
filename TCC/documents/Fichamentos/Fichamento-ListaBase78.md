# On Evaluating the Efficiency of Source Code Generated by LLMs

C. Niu, T. Zhang, C. Li, B. Luo, and V. Ng, "On Evaluating the Efficiency of Source Code Generated by LLMs," in *2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge)*, Lisbon, Portugal, 2024, pp. 103-107. doi: [10.1145/3650105.3652295](https://doi.org/10.1145/3650105.3652295)

## 1. Fichamento de Conteúdo

O artigo investiga a eficiência do código-fonte gerado por grandes modelos de linguagem (LLMs), indo além da avaliação tradicional de corretude. Os autores propõem uma avaliação empírica da eficiência utilizando três benchmarks principais: HumanEval, MBPP e um benchmark criado pelos próprios autores chamado LeetCodeEval. Os resultados indicam que a eficiência do código gerado não está diretamente correlacionada com a habilidade dos modelos em gerar código correto ou com o tamanho do modelo. O estudo mostra que modelos menores podem, em alguns casos, gerar código mais eficiente que modelos maiores. Além disso, diferentes estratégias de prompt podem influenciar significativamente a eficiência do código gerado, especialmente em problemas mais complexos, demonstrando a eficácia de prompts estruturados e detalhados (chain-of-thought).

## 2. Fichamento Bibliográfico

* _Eficiência Independente da Corretude_: Embora modelos mais poderosos frequentemente produzam código mais correto, essa característica não implica necessariamente em maior eficiência em execução, indicando que modelos com menos parâmetros podem gerar códigos mais eficientes (página 104).

* _Impacto das Estratégias de Prompt_: A eficiência pode ser significativamente melhorada através de diferentes estratégias de prompt, especialmente ao utilizar prompts que encorajam o raciocínio passo-a-passo (chain-of-thought) em problemas complexos (página 105).

* _Benchmarks de Avaliação_: HumanEval, MBPP e LeetCodeEval foram utilizados para avaliar a eficiência, sendo LeetCodeEval proposto pelos próprios autores para problemas mais complexos e diversificados, permitindo avaliações mais detalhadas sobre a eficiência dos códigos gerados pelos modelos (página 104).

## 3. Fichamento de Citações

* _"The efficiency of LLM-generated code is independent of the model’s performance on generating correct code and model size." (página 103)_

* _"Step-by-step prompting could make LLM generate more efficient code, especially on complex problems." (página 103)_

* _"GPT-4 generated code has the highest efficiency on average for complex LeetCodeEval problems, likely due to more diverse test cases." (página 104)_

* _"Prompt methods generally work better on LeetCodeEval than on simpler benchmarks due to extensive test cases that highlight efficiency differences." (página 106)_
